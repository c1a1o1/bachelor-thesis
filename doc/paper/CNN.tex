% $File: CNN.tex
% $Date: Wed Jun 03 23:00:43 2015 +0800
% $Author: jiakai <jia.kai66@gmail.com>

\chapter{基于深度卷积神经网络的特征提取}
本章主要介绍基于深度卷积神经网络的特征提取方法。基于不同的损失函数，
本章提出两个模型，
但其基本思想都是针对\secref{ISA:discuss}提到的ISA方法的局限性，
通过人工构造的损失函数，引导网络学习出对仿射变换和Gamma校正有鲁棒性、
同时具有较强区分性的特征。

\section{网络结构}
深度卷积神经网络一般由卷积、池化、全连接等层组成，
卷积和全连接层后一般接一个非线性函数。在本小节中，
先对这三种层对应的数学操作进行介绍，随后再给出本文所用的网络结构。

\subsection{基本数学操作}
在本小节中，我们对处理3D图像数据的深度卷积神经网络所用到的数学操作进行简单介绍。
一般而言，网络的输入是4D张量，在经过 若干卷积和池化后仍为4D张量，
保持空间信息，随后用全连接层进行降维，把4D张量转换为向量，最终得到期望的输出。
4D张量的维度用$(c, h, w, d)$表示，其中$c$为通道数，
$(h, w, d)$是在3D空间中各维对应的坐标。对于只有灰度值的3D图像块，其通道数$c=1$。

\subsubsection{卷积层}
卷积层的作用是在不同的空间位置用同一组特征检测器在输入数据上提取特征。
一般卷积层包含两个权重：卷积核和偏置量。在这里，
我们只考虑没有输入扩填(padding为0)且步长(stride)为$1$的情况。
设其输入维度为$(c_i, h_i, w_i, d_i)$，
卷积核的维度是$(c_o, c_i, h_k, w_k, d_k)$，
则输出的维度是$(c_o, h_i - h_k + 1, w_i - w_k + 1, d_i - d_k + 1)$，
而且要求偏置量是一个$c_o$维的向量。用$\vec{X}$表示输入，$\vec{Y}$表示输出，
$\vec{W}$表示卷积核，$\vec{b}$表示偏置量，则有如下关系：
\begin{eqnarray}
    \vec{Y}(c, x, y, z) = \sum_{k=0}^{c_i} \vec{X}(k) * \vec{W}(c, k) + b_c
\end{eqnarray}
其中$*$表示卷积运算，对于两个3D张量$\vec{A}, \vec{B}$，定义为
\begin{eqnarray}
    (\vec{A} * \vec{B})(i, j, k) = \sum_{x, y, z}\vec{A}(i-x, j-y, k-z)
        \vec{B}(x,y,z)
\end{eqnarray}
对$A$或$B$某维度的下标访问超出边界时，对应值按$0$处理。

对于卷积操作，可以理解为$\vec{W}$包含了$c_o$个特征检测器，
每个检测器的维度都是$(c_i, h_k, w_k, d_k)$，
与输入的$c_i$通道图像上对应的$h_k \times w_k \times d_k$图像块计算向量内积；
把这样的特征检测器重复应用到输入图像的所有空间位置得到输出图像，
输出图像有$c_o$个通道，每个空间位置都对应了一个$c_o$维的特征。

\subsubsection{池化层}
池化层的作用是增强特征的平移不变性，同时降低数据在空间上的维度。
一般有最大值池化(max pooling)、均值池化(mean pooling)等。
设池化操作的步长为$s$，核大小为$k$，
对于维度为$(c, h, w, d)$的输入图像$\vec{X}$，
输出图像$\vec{Y}$的维度为$(c, \floor{\frac{h-k}{s}+1},
\floor{\frac{w-k}{s}+1}, \floor{\frac{d-k}{s}+1})$，且满足
\begin{eqnarray}
    \vec{Y}(c, x, y, z) = f(X(c, sx:sx+k, sy:sy+k, sz:sz+k))
\end{eqnarray}
其中$f$为池化的操作，如取最大值、平均值等；
$\vec{X}(c, i_0:i_1, j_0:j_1, k_0:k_1)$
表示在$\vec{X}$的二、三、四维上分别取$[i_0, i_1), [j_0, j_1), [k_0, k_1)$
区间所对应的子张量。


\subsection{本文所用网络结构}
本文仅试验了一种网络结构，其计算层由三层卷积、一层池化、两层全连接组成。
为了与\secref{isa:expr}所描述的ISA模型公平比较，
输入与之相同用$21\times 21 \times 21$的图像块，输出同样为$50$维特征。
图像块在进入网络之前，
先要经过线性变换$\vec{y}=k\vec{x}+b$，其中$k$、$b$为在训练集上求得的标量常数，
使得$E[\vec{y}]=0$，$\max(\abs{\vec{y}})=1$。
网络中所有卷积和全连接层均使用使用ReLU作为非线性，即$f(x)=\max(x, 0)$。
具体的网络结构参数见\tabref{cnn:arch}。

\begin{table}[h!]
    \begin{center}
        \caption{本文所用的深度卷积神经网络结构}
        \label{tab:cnn:arch}
        \begin{tabular}{c|c|c}
            \hline
            {\heiti 层编号} & {\heiti 输入大小} & {\heiti 层内容} \\ \hline
            0 & $1\times 21 \times 21  \times 21$ & 线性变换：$y=kx+b$ \\
            1 & $1\times 21 \times 21  \times 21$ & $Conv(20, 4)$ \\
            2 & $20\times 18 \times 18  \times 18$ & $MeanPooling(2)$ \\
            3 & $20\times 9 \times 9 \times 9$ & $Conv(24, 4)$ \\
            4 & $24\times 6 \times 6 \times 6$ & $Conv(28, 4)$ \\
            5 & $28\times 3 \times 3 \times 3$ & $FC(60)$ \\
            6 & $60$ & $FC(50)$ \\
            7 & $50$ & 损失函数 \\
        \end{tabular}
    \end{center}
    \footnotesize
    注：
    \begin{center}
        \begin{itemize}
            \item $Conv(c, s)$ 表示输出通道数为$c$，
                卷积核大小为$s\times s \times s$，
                步长为$1$的3D卷积
            \item $MeanPooling(s)$ 表示大小为$s\times s \times s$
                且无重叠的均值池化
            \item $FC(n)$ 表示输出维度为$n$的全连接层
        \end{itemize}
    \end{center}
\end{table}

\section{损失函数}
\subsection{分类输出：Softmax与交叉熵损失函数}
\subsection{特征输出：度量学习}

\section{数据增广}
简要形式化描述希望特征对变换$F(A, \gamma)=(affine(A), gamma(\gamma))$
满足的不变性

\subsection{Gamma校正}

\subsection{三维仿射变换}

\section{训练方法}

% vim: filetype=tex foldmethod=marker foldmarker=f{{{,f}}}

